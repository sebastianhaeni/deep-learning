{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Approximation Theorem - Gradient Descent Optimisation\n",
    "\n",
    "Here we study the possibility to approximate functions with a MLP with a single hidden layer (n1 hidden units).\n",
    "As activation functions, we use the sigmoid ('logit') function.\n",
    "\n",
    "We generate training data - by assuming a function on the unit interval [0,1]. Here, we provide two families of functions:\n",
    "* Beta distribution function: $b_{\\alpha,\\beta}(x)=x^\\alpha\\cdot(1-x)^\\beta$\n",
    "* Sine function: $sin_\\omega(x)=\\sin(2\\pi\\omega\\cdot x)$\n",
    "\n",
    "Finally, we use mini-batch-gradient descent to minimize MSE cost.\n",
    "\n",
    "Goals:\n",
    "* Learn how a given function can be represented with a single layer MLP;\n",
    "* Understand that, in principle, it can be learned from sample data;\n",
    "* Understand that the optimization based on plain gradient is not always straightforward; \n",
    "* Experience that the choice of the hyper-parameters number of hidden units, batchsize, learning rate is tricky. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_function(x,y):\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('x')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_compare_function(x,y1,y2, label1='', label2=''):\n",
    "    plt.plot(x, y1, label=label1)\n",
    "    plt.xlabel('x')\n",
    "    plt.plot(x, y2, label=label2)\n",
    "    if label1 and label2:\n",
    "        plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,W1,b1,W2,b2):\n",
    "    \"\"\"\n",
    "    Computes the output for the single hidden layer network (n1 units) with 1d input and 1d output.\n",
    "    \n",
    "    Arguments:\n",
    "    W1 -- weights of the hidden layer with shape (n1,1)\n",
    "    b1 -- biases of the hidden units with shape (n1,1)\n",
    "    W2 -- weights of the output layer with shape (1,n1)\n",
    "    b2 -- bias of the output\n",
    "    X  -- input data with m samples and shape (1,m)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- Output from the network of shape (1,m) \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### START YOUR CODE ###\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array([0.4,0.2,-0.4]).reshape(3,1) # n1 = 3\n",
    "b1 = np.array([0.1,0.1,0.1]).reshape(3,1)\n",
    "W2 = np.array([1,2,1]).reshape(1,3)\n",
    "b2 = -1\n",
    "X = np.linspace(-1,1,5).reshape((1,5))\n",
    "Ypred = predict(X,W1,b1,W2,b2)\n",
    "Yexp = np.array([0.99805844, 1.04946333, 1.09991675, 1.14913132, 1.19690185]).reshape(1,5)\n",
    "np.testing.assert_array_almost_equal(Ypred,Yexp,decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(Y,Ypred):\n",
    "    \"\"\"\n",
    "    Computes the MSE cost for given ground truth Y and predicted Ypred\n",
    "    Uses the predict function defined above.\n",
    "    \n",
    "    Arguments:\n",
    "    Y -- ground truth output with shape (1,m) \n",
    "    Ypred -- predicted output with shape (1,m) \n",
    "    \n",
    "    Returns:\n",
    "    cost -- the MSE cost divided by 2.\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST - Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array([4,5,6]).reshape(3,1)\n",
    "W2 = np.array([1,2,3]).reshape(1,3)\n",
    "b1 = np.array([1,1,1]).reshape(3,1)\n",
    "b2 = 2\n",
    "X = np.linspace(-1,1,5).reshape(1,5)\n",
    "Ypred = predict(X,W1,b1,W2,b2)\n",
    "Y = 2.0*np.ones(5).reshape(1,5)\n",
    "c = cost(Y,Ypred)\n",
    "cexp = 9.01669099\n",
    "np.testing.assert_almost_equal(c,cexp,decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(W1,b1,W2,b2,X,Y):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the MSE cost for a single hidden layer network with 1d input and 1d output.\n",
    "    The parts of the gradient associated with the weights array and bias array for the hidden layer, \n",
    "    the weights array and the bias for the output layer are provided as separate numpy arrays of according \n",
    "    dimension. \n",
    "    \n",
    "    Arguments:    \n",
    "    W1 -- weights of hidden layer with shape (n1,1)\n",
    "    b1  -- biases of hidden layer with shape (n1,1)\n",
    "    W2 -- weights of output layer with shape (1,n1)\n",
    "    b2  -- biases of output layer\n",
    "    X  -- input data with shape (1,m)\n",
    "    Y  -- labels with shape (1,m)\n",
    "    \n",
    "    Returns:\n",
    "    gradient -- dictionary with the gradients w.r.t. W1, W2, b1, b2 and according keys \n",
    "                'dW1' with shape (n1,1)\n",
    "                'db1' with shape (n1,1)\n",
    "                'dW2' with shape (1,n1)\n",
    "                'db2' a scalar\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    return {'dW1':dW1, 'dW2':dW2, 'db1':db1, 'db2':db2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST - Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array([4,5,6]).reshape(3,1)\n",
    "W2 = np.array([1,2,3]).reshape(1,3)\n",
    "b1 = np.array([1,1,1]).reshape(3,1)\n",
    "b2 = 2\n",
    "X = np.array([1,2,3,4,5,6,7]).reshape((1,7))\n",
    "Y = np.array([2,2,2,2,2,2,2]).reshape((1,7))\n",
    "gradJ = gradient(W1,b1,W2,b2,X,Y)\n",
    "dW1exp = np.array([0.00590214,0.00427602,0.00234663]).reshape(W1.shape)\n",
    "db1exp = np.array([0.00579241,0.004247,0.00234079]).reshape(b1.shape)\n",
    "dW2exp = np.array([5.99209251,5.99579451,5.99714226]).reshape(W2.shape)\n",
    "db2exp = 5.99792323\n",
    "np.testing.assert_array_almost_equal(gradJ['dW1'],dW1exp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(gradJ['db1'],db1exp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(gradJ['dW2'],dW2exp,decimal=8)\n",
    "np.testing.assert_almost_equal(gradJ['db2'],db2exp,decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,Y,n1,nepochs,batchsize=32,learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Performs the training by using MBGD for a MLP with a single hidden layer (n1 units) and 1d input and output layer.\n",
    "    \n",
    "    It starts with initializing the parameters:\n",
    "    * the weights and the biases for the hidden units : W1,b1 of shape (n1,1) \n",
    "    * the weights and the bias for the output layer: W2 of shape (1,n1) and scalar b2 \n",
    "\n",
    "    Then, it loops over the epochs and per epoch over the mini-batches. The number of batches is determined from the \n",
    "    batchsize.\n",
    "    \"\"\"\n",
    "    # initialize weights\n",
    "    W1 = np.random.uniform(-1,1,n1).reshape(n1,1)*0.05\n",
    "    b1 = np.zeros((n1,1),dtype='float')\n",
    "    W2 = np.random.uniform(-1,1,n1).reshape(1,n1)*0.05\n",
    "    b2 = 0.0\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    mb = int(m/batchsize)\n",
    "    indices = np.arange(m)\n",
    "    #np.random.shuffle(indices)\n",
    "    \n",
    "    # remember the epoch id and cost after each epoch for constructing the learning curve at the end\n",
    "    costs = [] \n",
    "    epochs = []\n",
    "\n",
    "    # Initial cost value:\n",
    "    epochs.append(0)\n",
    "    Ypred = predict(X,W1,b1,W2,b2)\n",
    "    costs.append(cost(Y,Ypred)) \n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(nepochs):\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        epochs.append(epoch+1)\n",
    "        Ypred = predict(X,W1,b1,W2,b2)\n",
    "        costs.append(cost(Y,Ypred))         \n",
    "    \n",
    "    print(costs[-1])    \n",
    "    params = {'W1':W1, 'W2':W2,'b1':b1,'b2':b2}    \n",
    "    return params, np.array(epochs), np.array(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_fct(x,alpha,beta):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    x - input array\n",
    "    alpha, beta -- larger values lead to more pronounced peaks\n",
    "    \"\"\"\n",
    "    c = alpha/(alpha+beta)\n",
    "    norm = c**alpha*(1-c)**beta\n",
    "    return x**alpha*(1-x)**beta/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin_fct(x,omega):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    x -- input array\n",
    "    omega -- frequency that defines the integer number of cycles within the unit interval\n",
    "    \"\"\"\n",
    "    return np.sin(x*2*np.pi*omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs(m, func, random=True, vargs=None):\n",
    "    \"\"\"\n",
    "    Generates m (x,y=f(x))-samples by either generating random x-values in the unit interval (random=True) or by \n",
    "    generating a grid of such values. Then the y values (used as labels below) are created from the function object \n",
    "    `func`.\n",
    "    Parameter needed to define the function `func` can be passed as vargs-dict. \n",
    "    \"\"\"\n",
    "    if random:\n",
    "        x = np.random.rand(1,m)\n",
    "        y = func(x, **vargs)\n",
    "    else:\n",
    "        x = np.linspace(0,1,m).reshape(1,m)\n",
    "        y = func(x,**vargs)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000\n",
    "func = beta_fct\n",
    "vargs={'alpha':2.0,'beta':2.0}\n",
    "#func = sin_fct\n",
    "#vargs={'omega':3.0}\n",
    "\n",
    "X,Y = generate_inputs(m,func,vargs=vargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[0,:],Y[0,:],'+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Input and Output\n",
    "\n",
    "It turns out that it is important to normalize the input and the output data here.\n",
    "Remember the mean and stdev computed for the training data so that you can also apply it to the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X, mu=None, stdev=None):\n",
    "    \"\"\"\n",
    "    Normalizes the data by using z-normalization. If mu and stdev are NOT specified, mean and stedev are \n",
    "    computed from the given samples.   \n",
    "    \n",
    "    Returns:\n",
    "    X1 -- normalized data (array of the same shape as input)\n",
    "    mu -- mean\n",
    "    stdev -- standard deviation\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return X1,mu,stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_normalize(X1, mu, stdev):\n",
    "    \"\"\"\n",
    "    Invert the normalization. This is needed to bring the predicted values back to the original scale.\n",
    "\n",
    "    Returns:\n",
    "    X -- unnormalized data (array of the same shape as input X1)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Input Normalization\n",
    "x, _, _ = normalize(X)\n",
    "y, _, _ = normalize(Y)\n",
    "plt.plot(x[0,:],y[0,:],'+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the Training\n",
    "\n",
    "Includes generating and normalizing the data and training. Test data can be generated as non-random.<br>\n",
    "Make sure that you do the test performance on the right scales (of both x-values and y-values)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "mtrain = 1000\n",
    "mtest = 1000\n",
    "func = beta_fct\n",
    "vargs={'alpha':2.0,'beta':2.0}\n",
    "\n",
    "n_hidden = 10 # number of hidden units\n",
    "nepochs = 1000 # number of epochs\n",
    "batchsize = 32\n",
    "learning_rate = 0.1\n",
    "\n",
    "### START YOUR CODE ###\n",
    "\n",
    "# generate data (train and test)\n",
    "\n",
    "\n",
    "\n",
    "# normalize\n",
    "\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE ###\n",
    "\n",
    "plt.semilogy(epochs,costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "Compute the predicted values on the test set and compute the MSE cost.\n",
    "Prepare a (x,y)-plot with the ground truth test values and the predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "Ypred_test1 = ...\n",
    "Ypred_test = ...\n",
    "\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "plt.plot(Xtest[0,:],Ytest[0,:],'b-')\n",
    "plt.plot(Xtest[0,:],Ypred_test[0,:],'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
