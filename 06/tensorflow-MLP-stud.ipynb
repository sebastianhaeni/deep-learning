{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 24\n",
    "* Sebastian Häni <haeniseb@students.zhaw.ch>\n",
    "* Raffael Affolter <affolraf@students.zhaw.ch>\n",
    "* Benjamin Mäder <maedeben@students.zhaw.ch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with TensorFlow 2.0\n",
    "The objective of the exercise is to implement computational graphs in TensorFlow 2.0 to train and use such an architecture. The constraints we put ourselves is to use **low-level** functions of TensorFlow, i.e. we will not use high-level functions to compose layers and to train the parameters.\n",
    "\n",
    "If you get this error in the execution of the first cell: ` ModuleNotFoundError: No module named 'tensorflow' `, it probably means TensorFlow 2.0 is not installed yet on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST data set ready. N=60000, D=784, n_classes=10\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# MNIST Dataset Preparation #\n",
    "#############################\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train_vec),(x_test, y_test_vec) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train_vec, 10, dtype='float64')\n",
    "y_test = tf.keras.utils.to_categorical(y_test_vec, 10, dtype='float64')\n",
    "N = x_train.shape[0]         # number of samples\n",
    "D = x_train.shape[1]         # dimension of input sample\n",
    "n_classes = y_train.shape[1] # output dim\n",
    "print('MNIST data set ready. N={}, D={}, n_classes={}'.format(N,D,n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample a random batch from dataset\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0,len(data))  # create an array of index values\n",
    "    np.random.shuffle(idx)        # shuffle it\n",
    "    idx = idx[:num]               # take the first n indexes = size of batch\n",
    "    data_shuffle = data[idx]      # extract the batch using the random indexes\n",
    "    labels_shuffle = labels[idx]  # extract the labels using the random indexes\n",
    "\n",
    "    return data_shuffle, labels_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, loss = 56.779589521889804\n",
      "epoch = 1, loss = 41.356790575195\n",
      "epoch = 2, loss = 38.89203316187228\n",
      "epoch = 3, loss = 36.56298423621205\n",
      "epoch = 4, loss = 34.16185371817294\n",
      "epoch = 5, loss = 32.100150526652165\n",
      "epoch = 6, loss = 30.350133719945955\n",
      "epoch = 7, loss = 28.6506858683984\n",
      "epoch = 8, loss = 27.264919811442272\n",
      "epoch = 9, loss = 26.069121931890482\n",
      "epoch = 10, loss = 25.08069332488845\n",
      "epoch = 11, loss = 24.171745363913026\n",
      "epoch = 12, loss = 23.270033055487986\n",
      "epoch = 13, loss = 22.70107508259304\n",
      "epoch = 14, loss = 21.922857480720616\n",
      "epoch = 15, loss = 21.503554084472803\n",
      "epoch = 16, loss = 21.032036823210028\n",
      "epoch = 17, loss = 20.46524034540128\n",
      "epoch = 18, loss = 19.92825706532374\n",
      "epoch = 19, loss = 19.48326894804782\n",
      "epoch = 20, loss = 19.08408282246999\n",
      "epoch = 21, loss = 18.648095332484523\n",
      "epoch = 22, loss = 18.1946809178083\n",
      "epoch = 23, loss = 17.83742106029796\n",
      "epoch = 24, loss = 17.478190064235516\n",
      "epoch = 25, loss = 17.02389158005528\n",
      "epoch = 26, loss = 16.74837119989194\n",
      "epoch = 27, loss = 16.35878176616215\n",
      "epoch = 28, loss = 16.134793246793883\n",
      "epoch = 29, loss = 15.92416668785995\n",
      "epoch = 30, loss = 15.589121029911517\n",
      "epoch = 31, loss = 15.34126326367206\n",
      "epoch = 32, loss = 15.182304248144971\n",
      "epoch = 33, loss = 15.127283091827103\n",
      "epoch = 34, loss = 14.709539131330768\n",
      "epoch = 35, loss = 14.54681718615869\n",
      "epoch = 36, loss = 14.494886861601433\n",
      "epoch = 37, loss = 14.098839147310464\n",
      "epoch = 38, loss = 14.092616441467824\n",
      "epoch = 39, loss = 13.822245458655328\n",
      "epoch = 40, loss = 13.779285129576085\n",
      "epoch = 41, loss = 13.62584388160359\n",
      "epoch = 42, loss = 13.45372772612439\n",
      "epoch = 43, loss = 13.214067272318356\n",
      "epoch = 44, loss = 13.146456817959134\n",
      "epoch = 45, loss = 13.04121483074936\n",
      "epoch = 46, loss = 13.016209801395615\n",
      "epoch = 47, loss = 12.781080725507666\n",
      "epoch = 48, loss = 12.77896502805589\n",
      "epoch = 49, loss = 12.678147329216703\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Training phase #\n",
    "##################\n",
    "\n",
    "E = 50                # number of epochs\n",
    "B = 128               # batch size\n",
    "N = x_train.shape[0]  # number of samples\n",
    "D = x_train.shape[1]  # dimension of input sample\n",
    "H = 300               # number of neurons\n",
    "A = 0.01              # learning rate alpha\n",
    "\n",
    "##############################################\n",
    "#  COMPLETE CODE BELOW WHERE YOU SEE # ...   #\n",
    "##############################################\n",
    "\n",
    "# Build the computational graph\n",
    "@tf.function # this decorator tells tf that a graph is defined\n",
    "def mlp_train(x, y, alpha):\n",
    "    # define nodes for forward computation for hidden neurons h and output neurons y_pred\n",
    "    h = tf.nn.relu(tf.add(tf.matmul(x, w1, name='h_matmul'), b1, name='h_add'), name='h_max') # output of first layer after ReLu activation\n",
    "    y_pred = tf.sigmoid(tf.add(tf.matmul(h, w2, name='pred_matmul'), b2, name='pred_add'), name='pred_sigmoid') # output of second layer after sigmoid activation\n",
    "    # define nodes for forward computation for hidden neurons h and output neurons y_pred\n",
    "    diff = y_pred - y\n",
    "    loss = tf.reduce_mean(tf.pow(diff, 2))\n",
    "    # define the gradients\n",
    "    grad_w1, grad_b1, grad_w2, grad_b2 = tf.gradients(ys=loss, xs=[w1, b1, w2, b2])\n",
    "    # compute the new values of the gradients with the assign method (see slides)\n",
    "    w1.assign(w1 - alpha * grad_w1)\n",
    "    b1.assign(b1 - alpha * grad_b1)\n",
    "    w2.assign(w2 - alpha * grad_w2)\n",
    "    b2.assign(b2 - alpha * grad_b2)\n",
    "    return y_pred, loss\n",
    "\n",
    "# Init the tf.Variables w1, b1, w2, b2 following the given examples\n",
    "w1 = tf.Variable(tf.random.truncated_normal((D, H), stddev=0.1, dtype='float64'))\n",
    "b1 = tf.Variable(tf.constant(0.0, shape=[H], dtype='float64'))\n",
    "w2 = tf.Variable(tf.random.truncated_normal((H, 10), stddev=0.1, dtype='float64'))\n",
    "b2 = tf.Variable(tf.constant(0.0, shape=[10], dtype='float64'))\n",
    "\n",
    "# Run the computational graph\n",
    "J = []  # to store the evolution of loss J for each epoch\n",
    "                 \n",
    "for epoch in range(E):\n",
    "    J_epoch = 0.0\n",
    "    for _ in range(int(N/B)): # number of batches to visit for 1 epoch\n",
    "        # get batches calling the next_batch method provided above\n",
    "        x_train_batch, y_train_batch = next_batch(B, x_train, y_train)\n",
    "        with tf.device('/CPU:0'):  # change to /GPU:0 to move it to GPU\n",
    "            # call the graph with the batched input, target and alpha A\n",
    "            out = mlp_train(x_train_batch, y_train_batch, A)\n",
    "        y_pred, loss_val = out\n",
    "        J_epoch += loss_val\n",
    "    J.append(J_epoch)\n",
    "    print(\"epoch = {}, loss = {}\".format(epoch, J_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x655e29470>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the evolution of the loss\n",
    "plt.plot(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass on test set done.\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# Testing phase #\n",
    "#################\n",
    "\n",
    "N = x_test.shape[0]  # number of samples\n",
    "D = x_test.shape[1]  # dimension of input sample\n",
    "\n",
    "##############################################\n",
    "#  COMPLETE CODE BELOW WHERE YOU SEE # ...   #\n",
    "##############################################\n",
    "# Build the computational graph\n",
    "@tf.function # this decorator tells tf that a graph is defined\n",
    "def mlp_test(x, y):\n",
    "    h = tf.nn.relu(tf.add(tf.matmul(x, w1, name='h_matmul'), b1, name='h_add'), name='h_max') # output of first layer after ReLu activation\n",
    "    y_pred = tf.sigmoid(tf.add(tf.matmul(h, w2, name='pred_matmul'), b2, name='pred_add'), name='pred_sigmoid') # output of second layer after sigmoid activation\n",
    "    return y_pred\n",
    "\n",
    "# Run the computational graph\n",
    "with tf.device('/CPU:0'):  # change to /GPU:0 to move it to GPU\n",
    "    y_pred_test = mlp_test(x_test, y_test)\n",
    "\n",
    "print('Forward pass on test set done.')\n",
    "# At this stage, y_pred_test should contain the matrix of outputs on the test set with shape (N_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples  :  10000\n",
      "# correct  :  8779\n",
      "# missed   :  1221\n",
      "accuracy   :  87.79 %\n",
      "error rate :  12.21 %\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy\n",
    "y_winner = np.argmax(y_pred_test, axis=1)\n",
    "N_test = y_winner.size\n",
    "num_correct = (y_winner == y_test_vec).sum()\n",
    "num_missed = N_test - num_correct\n",
    "accuracy = num_correct * 1.0 / N_test\n",
    "error_rate = num_missed * 1.0 / N_test\n",
    "print('# samples  : ', N_test)\n",
    "print('# correct  : ', num_correct)\n",
    "print('# missed   : ', num_missed)\n",
    "print('accuracy   :  %2.2f %%'% (accuracy*100.0))\n",
    "print('error rate :  %2.2f %%'% (error_rate*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
