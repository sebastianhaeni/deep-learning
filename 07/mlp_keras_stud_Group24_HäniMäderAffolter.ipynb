{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 24\n",
    "\n",
    "• Sebastian Häni haeniseb@students.zhaw.ch\n",
    "• Raffael Affolter affolraf@students.zhaw.ch\n",
    "• Benjamin Mäder maedeben@students.zhaw.ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layersizes, drop_rate=0.0):\n",
    "    \"\"\"\n",
    "    Provides an MLP model (using Sequential) with given layersizes. The last layer is a softmax layer.\n",
    "    Add for the hidden layer a dropout layer. Make sure that the dropout layer is applied after the affiine transformation and before the activation function.\n",
    "    As activation function use sigmoid.\n",
    "        \n",
    "    Arguments:\n",
    "    layersizes -- list of integers with the number of hidden units per layer. The last element is for MNIST 10.\n",
    "    drop_rate -- the drop rate for dropout\n",
    "    \n",
    "    \"\"\"\n",
    "    ### START YOUR CODE HERE ###\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(layersizes[0], activation='relu'),\n",
    "    tf.keras.layers.Dense(layersizes[1], activation='relu'),\n",
    "    tf.keras.layers.Dense(layersizes[2], activation='relu'),\n",
    "    tf.keras.layers.Dense(layersizes[3], activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    ### STOP YOUR CODE HERE ###\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dokte\\Desktop\\MSE_Schuuuuule\\Semester_2\\DeepLearning\\Woche 7 Regularisation and optimisation\\excercise\\tb_logs_keras\\20191108-183608\\Baseline-trial\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 44,860\n",
      "Trainable params: 44,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 1.4374 - accuracy: 0.5805 - val_loss: 0.6155 - val_accuracy: 0.8336\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.4861 - accuracy: 0.8638 - val_loss: 0.3901 - val_accuracy: 0.8903\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.3659 - accuracy: 0.8954 - val_loss: 0.3257 - val_accuracy: 0.9046\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.3188 - accuracy: 0.9079 - val_loss: 0.2916 - val_accuracy: 0.9140\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.2891 - accuracy: 0.9165 - val_loss: 0.2729 - val_accuracy: 0.9202\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2667 - accuracy: 0.9230 - val_loss: 0.2507 - val_accuracy: 0.9279\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.2482 - accuracy: 0.9283 - val_loss: 0.2332 - val_accuracy: 0.9337\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.2325 - accuracy: 0.9336 - val_loss: 0.2241 - val_accuracy: 0.9351\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.2193 - accuracy: 0.9371 - val_loss: 0.2123 - val_accuracy: 0.9386\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2076 - accuracy: 0.9402 - val_loss: 0.1996 - val_accuracy: 0.9423\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1971 - accuracy: 0.9431 - val_loss: 0.1927 - val_accuracy: 0.9450\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.1882 - accuracy: 0.9457 - val_loss: 0.1852 - val_accuracy: 0.9464\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1798 - accuracy: 0.9483 - val_loss: 0.1779 - val_accuracy: 0.9476\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.1729 - accuracy: 0.9498 - val_loss: 0.1705 - val_accuracy: 0.9503\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.1656 - accuracy: 0.9521 - val_loss: 0.1654 - val_accuracy: 0.9530\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.1594 - accuracy: 0.9542 - val_loss: 0.1669 - val_accuracy: 0.9532\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1541 - accuracy: 0.9554 - val_loss: 0.1570 - val_accuracy: 0.9552\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1487 - accuracy: 0.9571 - val_loss: 0.1537 - val_accuracy: 0.9562\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1439 - accuracy: 0.9584 - val_loss: 0.1481 - val_accuracy: 0.9580\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1391 - accuracy: 0.9598 - val_loss: 0.1461 - val_accuracy: 0.9595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc26c8a548>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START YOUR CODE HERE ###\n",
    "from tensorflow.keras import layers\n",
    "layersizes = [50,50,50,10]\n",
    "\n",
    "epochs = 20\n",
    "batchsize = 100\n",
    "lr = 0.1\n",
    "drop_rate = 0.0\n",
    "run_name = \"Baseline-trial\"\n",
    "\n",
    "### STOP YOUR CODE HERE ###\n",
    "\n",
    "tensorboard_folder = \"tb_logs_keras\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outdir = os.path.join(os.getcwd(), tensorboard_folder, current_time, run_name)\n",
    "print(outdir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=outdir, histogram_freq=1, profile_batch=0)\n",
    "\n",
    "\n",
    "model = create_model(layersizes, drop_rate)    \n",
    "model.compile(optimizer=\"sgd\", learning_rate=lr, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x=x_train, y=y_train, epochs=epochs, batch_size=batchsize,\n",
    "          validation_data=(x_test, y_test), callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layersizes, drop_rate):\n",
    "    \"\"\"\n",
    "    Provides an MLP model (using Sequential) with given layersizes. The last layer is a softmax layer.\n",
    "    Add for the hidden layer a dropout layer. Make sure that the dropout layer is applied after the affiine transformation and before the activation function.\n",
    "    As activation function use sigmoid.\n",
    "        \n",
    "    Arguments:\n",
    "    layersizes -- list of integers with the number of hidden units per layer. The last element is for MNIST 10.\n",
    "    drop_rate -- the drop rate for dropout\n",
    "    \n",
    "    \"\"\"\n",
    "    ### START YOUR CODE HERE ###\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(layersizes[0], activation='relu'),\n",
    "    tf.keras.layers.Dense(layersizes[1], activation='relu'),\n",
    "    tf.keras.layers.Dense(layersizes[2], activation='relu'),\n",
    "    tf.keras.layers.Dropout(drop_rate),    \n",
    "    tf.keras.layers.Dense(layersizes[3], activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    ### STOP YOUR CODE HERE ###\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 1.7352 - accuracy: 0.4535 - val_loss: 0.7956 - val_accuracy: 0.8130\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.6559 - accuracy: 0.8059 - val_loss: 0.4118 - val_accuracy: 0.8884\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.4667 - accuracy: 0.8634 - val_loss: 0.3362 - val_accuracy: 0.9044\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.4006 - accuracy: 0.8838 - val_loss: 0.3048 - val_accuracy: 0.9129\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.3607 - accuracy: 0.8949 - val_loss: 0.2775 - val_accuracy: 0.9214\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.3337 - accuracy: 0.9045 - val_loss: 0.2586 - val_accuracy: 0.9244\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.3090 - accuracy: 0.9104 - val_loss: 0.2433 - val_accuracy: 0.9286\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.2932 - accuracy: 0.9147 - val_loss: 0.2294 - val_accuracy: 0.9341\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.2758 - accuracy: 0.9211 - val_loss: 0.2187 - val_accuracy: 0.9357\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.2614 - accuracy: 0.9254 - val_loss: 0.2117 - val_accuracy: 0.9374\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.2484 - accuracy: 0.9289 - val_loss: 0.1995 - val_accuracy: 0.9406\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.2347 - accuracy: 0.9322 - val_loss: 0.1896 - val_accuracy: 0.9447\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.2280 - accuracy: 0.9348 - val_loss: 0.1807 - val_accuracy: 0.9467\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.2171 - accuracy: 0.9377 - val_loss: 0.1729 - val_accuracy: 0.9492\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.2082 - accuracy: 0.9399 - val_loss: 0.1697 - val_accuracy: 0.9506\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1976 - accuracy: 0.9437 - val_loss: 0.1625 - val_accuracy: 0.9525\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1924 - accuracy: 0.9444 - val_loss: 0.1595 - val_accuracy: 0.9530\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1866 - accuracy: 0.9464 - val_loss: 0.1530 - val_accuracy: 0.9549\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.1803 - accuracy: 0.9483 - val_loss: 0.1477 - val_accuracy: 0.9561\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1731 - accuracy: 0.9498 - val_loss: 0.1436 - val_accuracy: 0.9573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc2858fac8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START YOUR CODE HERE ###\n",
    "from tensorflow.keras import layers\n",
    "layersizes = [50,50,50,10]\n",
    "\n",
    "epochs = 20\n",
    "batchsize = 100\n",
    "lr = 0.1\n",
    "drop_rate = 0.2\n",
    "run_name = \"Dropout_included\"\n",
    "\n",
    "### STOP YOUR CODE HERE ###\n",
    "\n",
    "tensorboard_folder = \"tb_logs_keras\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = os.path.join(os.getcwd(), tensorboard_folder, current_time, run_name)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)\n",
    "\n",
    "\n",
    "model = create_model(layersizes, drop_rate)    \n",
    "model.compile(optimizer=\"sgd\", learning_rate=lr, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x=x_train, y=y_train, epochs=epochs, batch_size=batchsize,\n",
    "          validation_data=(x_test, y_test), callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version with dropout achieves slightly higher accuracy in the end.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    if epoch < 21:\n",
    "        return 0.1\n",
    "    if epoch > 20 and epoch < 41:\n",
    "        return 0.05\n",
    "    if epoch > 40 and epoch < 71:\n",
    "        return 0.01\n",
    "        \n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 1.5950 - accuracy: 0.5184 - val_loss: 0.7245 - val_accuracy: 0.7998\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.5478 - accuracy: 0.8451 - val_loss: 0.4211 - val_accuracy: 0.8815\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.3920 - accuracy: 0.8864 - val_loss: 0.3503 - val_accuracy: 0.8962\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.3348 - accuracy: 0.9026 - val_loss: 0.3041 - val_accuracy: 0.9101\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.3013 - accuracy: 0.9126 - val_loss: 0.2756 - val_accuracy: 0.9204\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.2764 - accuracy: 0.9195 - val_loss: 0.2585 - val_accuracy: 0.9275\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2566 - accuracy: 0.9260 - val_loss: 0.2431 - val_accuracy: 0.9309\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2395 - accuracy: 0.9296 - val_loss: 0.2280 - val_accuracy: 0.9354\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.2247 - accuracy: 0.9349 - val_loss: 0.2193 - val_accuracy: 0.9365\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2122 - accuracy: 0.9379 - val_loss: 0.2035 - val_accuracy: 0.9398\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.2008 - accuracy: 0.9416 - val_loss: 0.1952 - val_accuracy: 0.9431\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.1913 - accuracy: 0.9448 - val_loss: 0.1885 - val_accuracy: 0.9440\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1826 - accuracy: 0.9474 - val_loss: 0.1838 - val_accuracy: 0.9462\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.1751 - accuracy: 0.9499 - val_loss: 0.1748 - val_accuracy: 0.9489\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.1678 - accuracy: 0.9510 - val_loss: 0.1721 - val_accuracy: 0.9490\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1615 - accuracy: 0.9529 - val_loss: 0.1647 - val_accuracy: 0.9512\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.1552 - accuracy: 0.9548 - val_loss: 0.1598 - val_accuracy: 0.9527\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1501 - accuracy: 0.9568 - val_loss: 0.1583 - val_accuracy: 0.9525\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1449 - accuracy: 0.9582 - val_loss: 0.1544 - val_accuracy: 0.9543\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.1404 - accuracy: 0.9590 - val_loss: 0.1480 - val_accuracy: 0.9565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21c6aad1f08>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START YOUR CODE HERE ###\n",
    "from tensorflow.keras import layers\n",
    "layersizes = [50,50,50,10]\n",
    "\n",
    "epochs = 20\n",
    "batchsize = 100\n",
    "lr = 0.1\n",
    "drop_rate = 0.0\n",
    "run_name = \"Changing_learningrate-newest\"\n",
    "\n",
    "### STOP YOUR CODE HERE ###\n",
    "\n",
    "tensorboard_folder = \"tb_logs_keras\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outdir = os.path.join(os.getcwd(), tensorboard_folder, current_time, run_name)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=outdir, histogram_freq=1, profile_batch=0)\n",
    "\n",
    "\n",
    "model = create_model(layersizes, drop_rate)    \n",
    "model.compile(optimizer=\"sgd\", learning_rate=lr, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x=x_train, y=y_train, epochs=epochs, batch_size=batchsize,\n",
    "          validation_data=(x_test, y_test), callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
